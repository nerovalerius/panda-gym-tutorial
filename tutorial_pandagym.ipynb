{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Reinforcement Learning - Tutorial - Panda-Gym\n",
    "### [Armin Niedermueller](https://github.com/nerovalerius)\n",
    "### Salzburg University of applied Sciences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Topics\n",
    "* Panda-Gym Introduction\n",
    "    * Franka Panda Robot\n",
    "    * Setup\n",
    "* Example Environment - Panda Reach\n",
    "* Create a custom robot\n",
    "* Custom a custom environment\n",
    "* Create a custom task\n",
    "    * Avoid Obstacles while reaching a target\n",
    "    * Stack 3 instead of 2 blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panda-Gym Introduction\n",
    "Panda-Gym is a reinforcement learning environment for the Franka Emika Panda robot. It is based on the OpenAI Gym framework and provides a set of tasks that can be used to train reinforcement learning agents. The tasks are based on the PyBullet physics engine and can be used to train agents for real-world applications. The environment is designed to be easy to use and extend. It is also possible to create custom robots, environments and tasks.\n",
    "A detailed documentation can be found [here](https://panda-gym.readthedocs.io/en/latest/) and their paper can be cited as folows:\n",
    "```\n",
    "@article{gallouedec2021pandagym,\n",
    "  title        = {{panda-gym: Open-Source Goal-Conditioned Environments for Robotic Learning}},\n",
    "  author       = {Gallou{\\'e}dec, Quentin and Cazin, Nicolas and Dellandr{\\'e}a, Emmanuel and Chen, Liming},\n",
    "  year         = 2021,\n",
    "  journal      = {4th Robot Learning Workshop: Self-Supervised and Lifelong Learning at NeurIPS},\n",
    "}\n",
    "```\n",
    "\n",
    "#### Franka Panda Robot\n",
    "Panda is a collaborative robot with 7 degrees of freedom developed by [FRANKA EMIKA](https://www.franka.de/).\n",
    "It can be programmed directly with a graphical user interface or with the Robot Operating System 1 & 2 (C++, MoveIt!, Rviz and so on).\n",
    "The torque sensors on it's 7 seven axes make this robot arm so sensitive, that it even stops at a balloon.\n",
    "It works at a very high precision as well as stability, which makes it a perfect tool for research and development.\n",
    "\n",
    "<img src=\"images/franka_panda.png\"  width=\"35%\"> \\\n",
    "Image source: [LINK](https://github.com/nerovalerius/collision_avoidance/blob/master/BAC2_niedermueller.pdf)\n",
    "\n",
    "I worked with the Panda robot for my bachelor thesis, where i used two 3D stereo cameras to enable collision avoidance for the robot arm. The robot was able to avoid obstacles while reaching a target. The code and results can be found [here](https://github.com/nerovalerius/collision_avoidance) and [here](https://www.youtube.com/watch?v=LQPS--bnvQY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Before we are able to start programming, we need to prepare our programming environment.\n",
    "First of all, we create a virtual environment for our undertaking, in order to avoid conflicts with other projects.\n",
    "We use the conda package manager to create a virtual environment. If you are not familiar with conda, you can find a tutorial [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "First, download and install miniconda.\n",
    "```\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
    "    && bash Miniconda3-latest-Linux-x86_64.sh -b \\\n",
    "    && rm Miniconda3-latest-Linux-x86_64.sh\n",
    "```\n",
    "Follow the instructions on the terminal and also initialize conda for your current shell.\n",
    "```\n",
    "conda init bash\n",
    "```\n",
    "Now, create a new environment and activate it.\n",
    "```\n",
    "conda create -n panda-gym-tutorial python=3.9\n",
    "```\n",
    "For panda-gym, there is currently no conda package available. Therefore, we need to install it with pip.\n",
    "Furthermore, we can install numpngw to store the rendered images as animated png files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install panda-gym\n",
    "%pip3 install numpngw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Environment - Panda Reach\n",
    "Panda-Gym defines for each task a separate environment. Let's take a look at the environment for the **Panda Reach** environment, where the robot has to reach a target position with its end-effector:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from panda_gym.envs.core import RobotTaskEnv\n",
    "from panda_gym.envs.robots.panda import Panda\n",
    "from panda_gym.envs.tasks.reach import Reach\n",
    "\n",
    "class PandaReachEnv(RobotTaskEnv):\n",
    "    \"\"\"Reach task wih Panda robot.\n",
    "    Args:\n",
    "        render (bool, optional): Activate rendering. Defaults to False.\n",
    "        reward_type (str, optional): \"sparse\" or \"dense\". Defaults to \"sparse\".\n",
    "        control_type (str, optional): \"ee\" to control end-effector position or \"joints\" to control joint values.\n",
    "            Defaults to \"ee\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, render: bool = False, reward_type: str = \"sparse\", control_type: str = \"ee\") -> None:\n",
    "        # use PyBullet as simulation backend\n",
    "        sim = PyBullet(render=render)\n",
    "        # use Panda robot, define its control type and initial position\n",
    "        robot = Panda(sim, block_gripper=True, base_position=np.array([-0.6, 0.0, 0.0]), control_type=control_type)\n",
    "        # use Reach task, define its reward type and initial end-effector position\n",
    "        task = Reach(sim, reward_type=reward_type, get_ee_position=robot.get_ee_position)\n",
    "        super().__init__(robot, task)\n",
    "```\n",
    "\n",
    "We define PyBullet as our physics engine for robotics. It is used to simulate and render the robot and the environment.\n",
    "\n",
    "The robot is defined in [/panda_gym/envs/robots/panda](https://github.com/qgallouedec/panda-gym/blob/master/panda_gym/envs/robots/panda.py). \\\n",
    "Here, you can find all necessary physics parameters (such as friction) and functions (such as set_action).\n",
    "\n",
    "Furthermore, the task is defined as Reach task, defined in [panda_gym/envs/tasks/reach.py](https://github.com/qgallouedec/panda-gym/blob/master/panda_gym/envs/tasks/reach.py).\\\n",
    "Inside this file, the 3D environment (such as the table) is defined and the reward is computed. \n",
    "\n",
    "Now, three parameters can be set in the environment:\n",
    "* **render:** activate or deactivate the rendering of the environment\n",
    "* **reward_type:**\n",
    "    * sparse: reward is 1 if the target is reached and 0 otherwise\n",
    "    * dense: reward is the distance between the target and the end-effector\n",
    "* **control_type:** actions should either control the robot's:\n",
    "    * end-effector position\n",
    "    * or joint values\n",
    "\n",
    "#### Code and Animation\n",
    "<img src=\"images/reach.png\"  width=\"35%\"> \n",
    "\n",
    "First, we create \"PandaReach-v3\" environment and set render to True, to see what the robot is learning. Then, we reset the environment, define our number of maximum number of episodes and let the robot take actions provided by our policy. After each step, we check if the episode is done and if so, we reset the environment. Beside the actual reinforcement learning, we also create an animation of the robot's learning process by storing the rendered images inside a png file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Task: Panda Reach               #\n",
    "###################################\n",
    "\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "from tqdm import tqdm\n",
    "from numpngw import write_apng\n",
    "\n",
    "# create environment and activate rendering\n",
    "env = gym.make(\"PandaReach-v3\", render=True)\n",
    "\n",
    "# define low frame rate for rendering to reduce computational load\n",
    "env.metadata['render_fps'] = 24\n",
    "\n",
    "# array to store images\n",
    "images = []\n",
    "\n",
    "# reset environment and get initial observation (either state of the joints or the end effector position)\n",
    "observation, info = env.reset()\n",
    "\n",
    "# define maximum number of episodes\n",
    "max_steps = 1000\n",
    "\n",
    "# run simulation \n",
    "for step in tqdm(range(max_steps)):\n",
    "    # take action as defined by our policy\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # execute action and get new observation, reward, termination flag and additional info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # add each image to our array for each step to create an animation afterwards\n",
    "    images.append(env.render('rgb_array'))\n",
    "\n",
    "    # when the episode is terminated, reset the environment\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save animation\n",
    "print(\"Saving animation...\")\n",
    "write_apng('images/reach.png', images, delay = 10)\n",
    "print(\"finished\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom robot\n",
    "This section follows the [panda-gym tutorial](https://panda-gym.readthedocs.io/en/latest/custom/custom_robot.html) for creating a custom robot. \n",
    "#### Create URDF\n",
    "First, we need to create a URDF file for our robot. We can use the [URDF Modeler](https://mymodelrobot.appspot.com/5629499534213120) to create a URDF file while seeing the robot in 3D.\n",
    "\n",
    "Video: https://www.theconstructsim.com/ros-projects-my-robotic-manipulator-02-urdf-xacro/\n",
    "https://www.theconstructsim.com/my-robotic-manipulator-1-basic-urdf-rviz/\n",
    "\n",
    "oder den hier umbauen: https://automaticaddison.com/how-to-build-a-simulated-robot-arm-using-ros/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom task\n",
    "\n",
    "https://panda-gym.readthedocs.io/en/latest/custom/custom_task.html\n",
    "\n",
    "#### Original Panda Stack task\n",
    "\n",
    "#### New task: Stack 3 blocks \n",
    "#### New task: avoid obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom environment\n",
    "https://panda-gym.readthedocs.io/en/latest/custom/custom_env.html\n",
    "\n",
    "### New environment: add obstacles to the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0c5f67b466fc5386e183f274ffb84e1509e89581f809d6c238ca5386dca0c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
